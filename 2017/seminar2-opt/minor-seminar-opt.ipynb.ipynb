{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Майнор по Анализу Данных, Группа ИАД-4\n",
    "## 21/09/2017 Методы Оптимизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print(u'Так надо')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Но в начале...\n",
    "Еще пару слов про SVM\n",
    "\n",
    "SVM позволяет встраивать собственные ядра! И это вам пригодится в домашке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Не помню, откуда взял этот код...\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "def my_kernel(U, V):\n",
    "    \"\"\"\n",
    "    We create a custom kernel:\n",
    "\n",
    "                 (2  0)\n",
    "    k(U, V) = U  (    ) V.T\n",
    "                 (0  1)\n",
    "    \"\"\"\n",
    "    M = np.array([[2, 0], [0, 1.0]])\n",
    "    return np.dot(np.dot(U, M), V.T)\n",
    "\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# we create an instance of SVM and fit out data.\n",
    "clf = svm.SVC(kernel=my_kernel)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title('3-Class classification using Support Vector Machine with custom'\n",
    "          ' kernel')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы оптимизации\n",
    "\n",
    "Как было показано на лекции, большинство методов машинного обучения сводятся к поиску параметров, которые минимизируют ошибку на тренировочной выборке:\n",
    "$$\n",
    "\\min_{\\beta} L(\\beta; D)\n",
    "$$\n",
    "Здесь:\n",
    "* $D$ — размеченная обучающая выборка, $\\{x^{(i)}, y^{(i)}\\}_{i=1}^N$\n",
    "* $L$ — функция потерь\n",
    "* $\\beta$ — настраиваемые веса алгоритма\n",
    "\n",
    "В более общем виде задачу можно записать так:\n",
    "$$\n",
    "\\min_{x} f(x)\n",
    "$$\n",
    "Здесь:\n",
    "* $x$ — вектор значений\n",
    "* $f$ — функция, принимающая вектор в качестве аргумента и выдающая числовое значение.\n",
    "\n",
    "На семинаре рассмотрим подробнее методы минимизации функции, которые рассматривались на лекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "\n",
    "Для оптимизации возьмем простую функцию $f(x) = x^3 - 2x^2 + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x ** 3 - 2*x ** 2 + 2\n",
    "df = lambda x: 3 * x ** 2 - 4 * x # производная\n",
    "x = np.linspace(-1, 2.5, 1000)\n",
    "plt.plot(x, f(x))\n",
    "plt.xlim([-1, 2.5])\n",
    "plt.ylim([0, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И определим функцию, которая будет оптимизировать функцию $f(x)$ градиентным спуском с заданным постоянным шагом (он же learning rate, темп обучения)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_and_plot_steps(learning_rate, x_new=2, compute_learning_rate=None):\n",
    "    x_old = 0\n",
    "    # x_new — точка старта\n",
    "    eps = 0.0001\n",
    "    x_list, y_list = [x_new], [f(x_new)] # инициализируем список координат и значений функций при итерации\n",
    "    \n",
    "    # спускаемся, пока разница между координатами не достигла требуемой точности\n",
    "    i = 0\n",
    "    while abs(x_new - x_old) > eps: \n",
    "        x_old = x_new\n",
    "        # считаем направление спуска\n",
    "        direction = -df(x_old)\n",
    "        # обновляем значение темпа обучения, если нам задана функция для этого\n",
    "        if compute_learning_rate is not None:\n",
    "            learning_rate = compute_learning_rate(i, learning_rate)\n",
    "        # делаем шаг\n",
    "        x_new = x_old + learning_rate * direction\n",
    "        # запоминаем очередной шаг минимизации\n",
    "        x_list.append(x_new)\n",
    "        y_list.append(f(x_new))\n",
    "        i += 1\n",
    "        \n",
    "    print \"Найденный локальный минимум:\", x_new\n",
    "    print \"Количество шагов:\", len(x_list)\n",
    "    \n",
    "    plt.figure(figsize=[10,3])\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(x_list, y_list, c=\"r\")\n",
    "    plt.plot(x_list, y_list, c=\"r\")\n",
    "    plt.plot(x, f(x), c=\"b\")\n",
    "    plt.xlim([-1,2.5])\n",
    "    plt.ylim([0,3])\n",
    "    plt.title(\"Descent trajectory\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(x_list,y_list,c=\"r\")\n",
    "    plt.plot(x_list,y_list,c=\"r\")\n",
    "    plt.plot(x,f(x), c=\"b\")\n",
    "    plt.xlim([1.2,2.1])\n",
    "    plt.ylim([0,3])\n",
    "    plt.title(\"Descent trajectory (zoomed in)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем оптимизацию с шагом 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем шаг побольше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что, если взять 0.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Застопорились в нуле, т.к. нашли точный локальный максимум. В нем производная равна нулю и мы никуда не можем сдвинуться. А если взять 0.49?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что, если взять 0.51?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы улетели далеко влево. Это можно понять, распечатав значения x_new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмём маленький шаг. Например, 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.01?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем меньше шаг, тем медленнее мы идём к минимум (и можем вдобавок застрять по пути). Чем больше темп обучения, тем большие расстояния мы перепрыгиваем (и имеем гипотетическую возможность найти минимум получше). Хорошая стратегия — начинать с достаточно большого шага (чтобы хорошо попутешествовать по функции), а потом постепенно его уменьшать, чтобы стабилизировать процесс обучения в каком-то локальном минимуме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем изменять шаг динамически:\n",
    "$lr(i + 1) = lr(i) * 0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_learning_rate(i, prev_lr):\n",
    "    return prev_lr * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.4, compute_learning_rate=compute_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если сравнивать с постоянным темпом обучения, то мы нашли минимум в 2 раза быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_and_plot_steps(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это, конечно, искуственный пример, но такая же идея используются для обучения алгоритмов машинного обучения с миллионами параметров, функции потерь которых имеют очень сложную структуру и не поддаются визуализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, iters, alpha):\n",
    "    costs = []\n",
    "    n = y.shape[0] \n",
    "    Beta = np.random.rand(2)\n",
    "    history = [Beta] \n",
    "    preds = []\n",
    "    for i in xrange(iters):\n",
    "        y_hat = X.dot(Beta)\n",
    "        error = y_hat - y \n",
    "        cost = np.mean(error ** 2) / 2\n",
    "        \n",
    "        if i % 100 == 0: \n",
    "            preds.append(y_hat)\n",
    "            history.append(Beta)\n",
    "            costs.append(cost)\n",
    "\n",
    "        gradient = X.T.dot(error)\n",
    "        Beta = Beta - alpha / n * gradient  # update\n",
    "        \n",
    "        \n",
    "    return history, costs, preds, Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = gradient_descent(X, y, 1000, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск для линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание задачи линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним суть метода градиентого спуска в контексте задачи линейной регрессии.\n",
    "\n",
    "Дано описание $n$ объектов по $m$ признакам. Обычно оно выражается в виде матрицы размера $n \\times m$: $X = [x^{(i)}_j]^{i=1\\dots n}_{j=1\\dots m} $.<br\\> ($x^{(i)}_j$ означает $j$-ый признак $i$-го объекта) <br\\>\n",
    "Дана зависимая переменная, которая тоже имеет отношение к этим объекам: $y$ - вектор длины $n$.\n",
    "\n",
    "Наша задача, выявить **линейную** зависимость между признаками в $X$ и значениями в $y$:\n",
    "$$\\hat{y} = X\\beta \\quad \\Leftrightarrow \\quad \\hat{y}^{(i)} = \\beta_0 + \\beta_1x^{(i)}_1 + \\dots$$\n",
    "\n",
    "И сделать это так, чтобы квадрат суммы ошибок наших оценок был минимален:\n",
    "$$ L(\\beta) = \\frac{1}{2n}(\\hat{y} - y)^{\\top}(\\hat{y} - y) = \\frac{1}{2n}(X\\beta - y)^{\\top}(X\\beta - y) \\rightarrow \\min$$ $$ \\Updownarrow $$  $$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 + \\dots - y^{(i)})^2  \\rightarrow \\min $$\n",
    "\n",
    "Значение в $X$ и $y$ нам даны. Нам неизвестны только значения коэффициентов $\\beta$.<br\\> Соответственно, нужно найти такие значения $\\beta$, что функция $L(\\beta) \\rightarrow \\min.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание метода градиентного спуска для регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть нам известнен 1 признак объекта и мы включаем свободный член у ровнение регрессии.\n",
    "\n",
    "Посчитаем, чему равен градиент функции потерь $L(\\beta_0, \\beta_1):$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_0} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})$$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_1} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})x_1^{(i)}$$\n",
    "\n",
    "Иногда проще это записать в виде матриц:\n",
    "$$ \\nabla_\\beta L(\\beta) = X^\\top(X\\beta - y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод градиентного спуска заключается в итеративном и **одновременном(!!!)** обновлении значений $\\beta$ в направлении, противоположному градиенту:\n",
    "$$ \\beta := \\beta - \\alpha\\nabla_\\beta L(\\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пошаговое описание программной реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала нам понадобятся данные. \n",
    "\n",
    "Используем те же данные про грузовики, что и были даны на втором семинаре. Нам дано два столбца значений — количество жителей в городе и доход грузовика с уличной едой в этом городе.\n",
    "\n",
    "Будем строить модель, описывающую зависимость дохода от размера населения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ./data/food_trucks.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join('data', 'food_trucks.txt')\n",
    "data = np.loadtxt(filepath, delimiter=',')\n",
    "plt.scatter(data[:,0], data[:, 1])\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Тогда X - это будет матрица размера ( 97x2 ), a y - вектор-столбец\n",
    "\n",
    "# Отнормируем данные\n",
    "data[:, 0] = (data[:, 0] - data[:, 0].mean())/data[:, 0].std()\n",
    "X = np.c_[data[:, 0], np.ones((data.shape[0], 1))]\n",
    "\n",
    "y = data[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь научимся считать ошибку модели. Для этого нам нужны коэффициенты. \n",
    "Пока возьмем некоторые случайные значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Beta = np.array([5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было показано выше: \n",
    "$$ \\hat{y}^{(i)} = \\beta_0 + \\beta_1x^{(i)}_1 + \\dots \\quad \\Leftrightarrow \\quad \\hat{y} = X\\beta \\quad \\Leftrightarrow \\quad \\texttt{y_hat = X.dot(Beta)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = X.dot(Beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, ошибка это $(X\\beta - y)$, а функция потерь $ L(\\beta) = \\frac{1}{2n}(X\\beta - y)^{\\top}(X\\beta - y) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 + \\dots - y^{(i)})^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = (X.dot(Beta) - y)\n",
    "\n",
    "n = X.shape[0]\n",
    "cost = np.sum(error ** 2) / (2 * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы научились считать ошибку при заданных $\\beta$. Теперь выразим градиент:\n",
    "\n",
    "$$ \\nabla_\\beta L(\\beta)= X^\\top(X\\beta - y) \\quad \\Leftrightarrow \\quad  \\texttt{grad = X.T.dot(error)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь к шагам алгоритма:\n",
    "\n",
    "* Задаем случайное начальное значение для $\\beta$\n",
    "* Пока не будет достигнуто правило останова:\n",
    "    * Считаем ошибку и значение функции потерь\n",
    "    * Считаем градиент\n",
    "    * Обновляем коэффициенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, iters, alpha):\n",
    "    n = y.shape[0] \n",
    "    Beta = np.random.rand(2)\n",
    "    for i in xrange(iters):\n",
    "        y_hat = X.dot(Beta)\n",
    "        \n",
    "        # считаем ошибку и значение функции потерь\n",
    "        error = y_hat - y \n",
    "        cost = np.sum(error ** 2) / (2 * n)\n",
    "\n",
    "        # считаем градиент\n",
    "        gradient = X.T.dot(error)\n",
    "\n",
    "        # обновляем коэффициенты\n",
    "        Beta = Beta - alpha / n * gradient  # update\n",
    "    return Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta = gradient_descent(X, y, 1000, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразим функцию потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возьмем значения для коэффициентов на интервале от 0 до 10\n",
    "beta0, beta1 = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 10, 100))\n",
    "BetaArr = np.r_[beta0.reshape(1,-1), beta1.reshape(1,-1)]\n",
    "\n",
    "# Посчитаем ошибку от всевозможных паросочетаний\n",
    "y_hat = X.dot(BetaArr)\n",
    "error = y_hat - y.reshape(-1,1)\n",
    "cost = np.sum(error ** 2, 0) / (2 * n)\n",
    "cost = cost.reshape(beta0.shape)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "contour = plt.contour(beta0, beta1, cost)\n",
    "plt.clabel(contour, inline=1, fontsize=10)\n",
    "plt.xlabel('$\\\\beta_0$')\n",
    "plt.ylabel('$\\\\beta_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Добавьте правило останова, срабатывающее при слабом изменении функции потерь\n",
    "* Измените функцию градиентного спуска так, чтобы темп обучения мог меняться динамически \n",
    "* Дополните функцию, чтобы она так же выводила значения коэффициентов и функции потерь на некоторых итерациях. Выведите \"прогресс\" спуска на график выше\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск с батч-оптимизацией\n",
    "\n",
    "Теперь рассмотрим случай, когда данных в выборке много. \n",
    "\n",
    "В таких случаях используется стохастическая или батч-оптимизация. Первая состоит в том, что на каждом шаге итерации берется один объект, вторая — в том, что берется некоторое небольшое фиксированное количество объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите данные из файла `space_ga.csv` и нормализуйте их. Мы будем предсказывать первый столбец по шести остальным. Эти данные получены с выборов в США в 1980 году. Подробнее о столбцах можно прочитать [тут](http://mldata.org/repository/data/viewslug/statlib-20050214-space_ga/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы могли заметить, датасет больше предыдущего. На нём мы попробуем батч-оптимизацию.\n",
    "\n",
    "## Задание\n",
    "Измените функцию для градиентного спуска так, чтобы на вход они принимала дополнительный параметр — размер батча. Для простоты проверки рекомендуется изменять копию функции, реализованной выше, с измененным именем. Прокомментируйте результаты.\n",
    "\n",
    "**Замечания**<br/>\n",
    "* Объекты нужно сначала перемешать (`sklearn.utils.shuffle`), а затем разделить на батчи\n",
    "* Учитите, что ошибка (и, соответственно, градиент) считается об объектам попавшим в батч, а значение функции потерь - по всем объектам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Другие изощренные модификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод накопленного импульса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_t = \\gamma v_{t-1} + \\alpha\\nabla_\\beta L(\\beta)$$\n",
    "$$ \\beta = \\beta - v_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод Нестерова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_t = \\gamma v_{t-1} + \\alpha\\nabla_\\beta L(\\beta - \\gamma v_{t-1})$$\n",
    "$$ \\beta = \\beta - v_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Адаптивный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g_t$ - производная по одному из параметров $\\frac{\\partial L}{\\partial \\beta_j}$\n",
    "\n",
    "$$ G_t = G_t + g_t^2$$\n",
    "$$ \\beta_j = \\beta_j - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} g_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
